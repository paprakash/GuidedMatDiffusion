{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "import pytorch_lightning as pl\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pytorch_lightning import seed_everything, Callback\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from diffcsp.common.utils import log_hyperparameters, PROJECT_ROOT\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_callbacks(cfg: DictConfig) -> List[Callback]:\n",
    "    callbacks: List[Callback] = []\n",
    "\n",
    "    if \"lr_monitor\" in cfg.logging:\n",
    "        hydra.utils.log.info(\"Adding callback <LearningRateMonitor>\")\n",
    "        callbacks.append(\n",
    "            LearningRateMonitor(\n",
    "                logging_interval=cfg.logging.lr_monitor.logging_interval,\n",
    "                log_momentum=cfg.logging.lr_monitor.log_momentum,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if \"early_stopping\" in cfg.train:\n",
    "        hydra.utils.log.info(\"Adding callback <EarlyStopping>\")\n",
    "        callbacks.append(\n",
    "            EarlyStopping(\n",
    "                monitor=cfg.train.monitor_metric,\n",
    "                mode=cfg.train.monitor_metric_mode,\n",
    "                patience=cfg.train.early_stopping.patience,\n",
    "                verbose=cfg.train.early_stopping.verbose,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if \"model_checkpoints\" in cfg.train:\n",
    "        hydra.utils.log.info(\"Adding callback <ModelCheckpoint>\")\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                dirpath=Path(HydraConfig.get().run.dir),\n",
    "                monitor=cfg.train.monitor_metric,\n",
    "                mode=cfg.train.monitor_metric_mode,\n",
    "                save_top_k=cfg.train.model_checkpoints.save_top_k,\n",
    "                verbose=cfg.train.model_checkpoints.verbose,\n",
    "                save_last=cfg.train.model_checkpoints.save_last,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/blue/hennig/pawanprakash/diffusion/MaterialsDiffusion')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/44383126/ipykernel_3793186/2186451020.py:4: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=str(PROJECT_ROOT / \"conf\"), config_name=\"default\")\n"
     ]
    }
   ],
   "source": [
    "cfg = None  # Declare cfg globally\n",
    "\n",
    "\n",
    "@hydra.main(config_path=str(PROJECT_ROOT / \"conf\"), config_name=\"default\")\n",
    "def main(config: DictConfig):\n",
    "    global cfg\n",
    "    cfg = config\n",
    "    run(cfg)\n",
    "    return cfg\n",
    "\n",
    "def run(cfg: DictConfig):\n",
    "    # Use cfg as needed\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawanprakash/miniconda3/envs/fermat/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/blue/hennig/pawanprakash/diffusion/MaterialsDiffusion/diffcsp/pl_data/dataset.py:155: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=str(PROJECT_ROOT / \"conf\"), config_name=\"default\")\n"
     ]
    }
   ],
   "source": [
    "from diffcsp.pl_data.dataset import CrystDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CrystDataset('vall',path = '/blue/hennig/pawanprakash/diffusion/MaterialsDiffusion/data/mp_20/val.csv',\n",
    "                       prop='band_gap',niggli=True,primitive=True,graph_method='crystalnn',preprocess_workers=1,\n",
    "                       lattice_scale_method='scale_length',save_path='/blue/hennig/pawanprakash/diffusion/MaterialsDiffusion/data/mp_20/val_ori.pt',\n",
    "                       tolerance=0.1,use_space_group=False,use_pos_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "itd = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'band_gap'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 32], y=[1, 1], frac_coords=[4, 3], atom_types=[4], lengths=[1, 3], angles=[1, 3], to_jimages=[32, 3], num_atoms=4, num_bonds=32, num_nodes=4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(itd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Data' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39mnext\u001b[39;49m(itd))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Data' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(next(itd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffcsp.common.data_utils import get_scaler_from_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/hennig/pawanprakash/diffusion/MaterialsDiffusion/diffcsp/common/data_utils.py:1119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "dataset.scaler = get_scaler_from_data_list(dataset.cached_data,key='band_gap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawanprakash/miniconda3/envs/fermat/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "dataloader = DataLoader(dataset,batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScalerTorch(means: inf, stds: nan)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tcad'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 DataBatch(edge_index=[2, 410], y=[10, 1], frac_coords=[46, 3], atom_types=[46], lengths=[10, 3], angles=[10, 3], to_jimages=[410, 3], num_atoms=[10], num_bonds=[10], num_nodes=46, batch=[46], ptr=[11])\n",
      "1 DataBatch(edge_index=[2, 406], y=[10, 1], frac_coords=[42, 3], atom_types=[42], lengths=[10, 3], angles=[10, 3], to_jimages=[406, 3], num_atoms=[10], num_bonds=[10], num_nodes=42, batch=[42], ptr=[11])\n",
      "2 DataBatch(edge_index=[2, 478], y=[10, 1], frac_coords=[48, 3], atom_types=[48], lengths=[10, 3], angles=[10, 3], to_jimages=[478, 3], num_atoms=[10], num_bonds=[10], num_nodes=48, batch=[48], ptr=[11])\n",
      "3 DataBatch(edge_index=[2, 448], y=[10, 1], frac_coords=[45, 3], atom_types=[45], lengths=[10, 3], angles=[10, 3], to_jimages=[448, 3], num_atoms=[10], num_bonds=[10], num_nodes=45, batch=[45], ptr=[11])\n",
      "4 DataBatch(edge_index=[2, 488], y=[10, 1], frac_coords=[48, 3], atom_types=[48], lengths=[10, 3], angles=[10, 3], to_jimages=[488, 3], num_atoms=[10], num_bonds=[10], num_nodes=48, batch=[48], ptr=[11])\n",
      "5 DataBatch(edge_index=[2, 384], y=[10, 1], frac_coords=[48, 3], atom_types=[48], lengths=[10, 3], angles=[10, 3], to_jimages=[384, 3], num_atoms=[10], num_bonds=[10], num_nodes=48, batch=[48], ptr=[11])\n",
      "6 DataBatch(edge_index=[2, 436], y=[10, 1], frac_coords=[41, 3], atom_types=[41], lengths=[10, 3], angles=[10, 3], to_jimages=[436, 3], num_atoms=[10], num_bonds=[10], num_nodes=41, batch=[41], ptr=[11])\n",
      "7 DataBatch(edge_index=[2, 394], y=[10, 1], frac_coords=[45, 3], atom_types=[45], lengths=[10, 3], angles=[10, 3], to_jimages=[394, 3], num_atoms=[10], num_bonds=[10], num_nodes=45, batch=[45], ptr=[11])\n",
      "8 DataBatch(edge_index=[2, 518], y=[10, 1], frac_coords=[50, 3], atom_types=[50], lengths=[10, 3], angles=[10, 3], to_jimages=[518, 3], num_atoms=[10], num_bonds=[10], num_nodes=50, batch=[50], ptr=[11])\n",
      "9 DataBatch(edge_index=[2, 442], y=[10, 1], frac_coords=[44, 3], atom_types=[44], lengths=[10, 3], angles=[10, 3], to_jimages=[442, 3], num_atoms=[10], num_bonds=[10], num_nodes=44, batch=[44], ptr=[11])\n",
      "10 DataBatch(edge_index=[2, 506], y=[10, 1], frac_coords=[52, 3], atom_types=[52], lengths=[10, 3], angles=[10, 3], to_jimages=[506, 3], num_atoms=[10], num_bonds=[10], num_nodes=52, batch=[52], ptr=[11])\n",
      "11 DataBatch(edge_index=[2, 386], y=[10, 1], frac_coords=[43, 3], atom_types=[43], lengths=[10, 3], angles=[10, 3], to_jimages=[386, 3], num_atoms=[10], num_bonds=[10], num_nodes=43, batch=[43], ptr=[11])\n",
      "12 DataBatch(edge_index=[2, 346], y=[10, 1], frac_coords=[39, 3], atom_types=[39], lengths=[10, 3], angles=[10, 3], to_jimages=[346, 3], num_atoms=[10], num_bonds=[10], num_nodes=39, batch=[39], ptr=[11])\n",
      "13 DataBatch(edge_index=[2, 432], y=[10, 1], frac_coords=[47, 3], atom_types=[47], lengths=[10, 3], angles=[10, 3], to_jimages=[432, 3], num_atoms=[10], num_bonds=[10], num_nodes=47, batch=[47], ptr=[11])\n",
      "14 DataBatch(edge_index=[2, 526], y=[10, 1], frac_coords=[50, 3], atom_types=[50], lengths=[10, 3], angles=[10, 3], to_jimages=[526, 3], num_atoms=[10], num_bonds=[10], num_nodes=50, batch=[50], ptr=[11])\n",
      "15 DataBatch(edge_index=[2, 366], y=[10, 1], frac_coords=[39, 3], atom_types=[39], lengths=[10, 3], angles=[10, 3], to_jimages=[366, 3], num_atoms=[10], num_bonds=[10], num_nodes=39, batch=[39], ptr=[11])\n",
      "16 DataBatch(edge_index=[2, 416], y=[10, 1], frac_coords=[47, 3], atom_types=[47], lengths=[10, 3], angles=[10, 3], to_jimages=[416, 3], num_atoms=[10], num_bonds=[10], num_nodes=47, batch=[47], ptr=[11])\n",
      "17 DataBatch(edge_index=[2, 474], y=[10, 1], frac_coords=[48, 3], atom_types=[48], lengths=[10, 3], angles=[10, 3], to_jimages=[474, 3], num_atoms=[10], num_bonds=[10], num_nodes=48, batch=[48], ptr=[11])\n",
      "18 DataBatch(edge_index=[2, 518], y=[10, 1], frac_coords=[49, 3], atom_types=[49], lengths=[10, 3], angles=[10, 3], to_jimages=[518, 3], num_atoms=[10], num_bonds=[10], num_nodes=49, batch=[49], ptr=[11])\n",
      "19 DataBatch(edge_index=[2, 512], y=[10, 1], frac_coords=[51, 3], atom_types=[51], lengths=[10, 3], angles=[10, 3], to_jimages=[512, 3], num_atoms=[10], num_bonds=[10], num_nodes=51, batch=[51], ptr=[11])\n",
      "20 DataBatch(edge_index=[2, 422], y=[10, 1], frac_coords=[45, 3], atom_types=[45], lengths=[10, 3], angles=[10, 3], to_jimages=[422, 3], num_atoms=[10], num_bonds=[10], num_nodes=45, batch=[45], ptr=[11])\n",
      "21 DataBatch(edge_index=[2, 520], y=[10, 1], frac_coords=[50, 3], atom_types=[50], lengths=[10, 3], angles=[10, 3], to_jimages=[520, 3], num_atoms=[10], num_bonds=[10], num_nodes=50, batch=[50], ptr=[11])\n",
      "22 DataBatch(edge_index=[2, 436], y=[10, 1], frac_coords=[48, 3], atom_types=[48], lengths=[10, 3], angles=[10, 3], to_jimages=[436, 3], num_atoms=[10], num_bonds=[10], num_nodes=48, batch=[48], ptr=[11])\n",
      "23 DataBatch(edge_index=[2, 410], y=[10, 1], frac_coords=[42, 3], atom_types=[42], lengths=[10, 3], angles=[10, 3], to_jimages=[410, 3], num_atoms=[10], num_bonds=[10], num_nodes=42, batch=[42], ptr=[11])\n",
      "24 DataBatch(edge_index=[2, 546], y=[10, 1], frac_coords=[49, 3], atom_types=[49], lengths=[10, 3], angles=[10, 3], to_jimages=[546, 3], num_atoms=[10], num_bonds=[10], num_nodes=49, batch=[49], ptr=[11])\n",
      "25 DataBatch(edge_index=[2, 448], y=[10, 1], frac_coords=[50, 3], atom_types=[50], lengths=[10, 3], angles=[10, 3], to_jimages=[448, 3], num_atoms=[10], num_bonds=[10], num_nodes=50, batch=[50], ptr=[11])\n",
      "26 DataBatch(edge_index=[2, 432], y=[10, 1], frac_coords=[50, 3], atom_types=[50], lengths=[10, 3], angles=[10, 3], to_jimages=[432, 3], num_atoms=[10], num_bonds=[10], num_nodes=50, batch=[50], ptr=[11])\n",
      "27 DataBatch(edge_index=[2, 412], y=[10, 1], frac_coords=[38, 3], atom_types=[38], lengths=[10, 3], angles=[10, 3], to_jimages=[412, 3], num_atoms=[10], num_bonds=[10], num_nodes=38, batch=[38], ptr=[11])\n",
      "28 DataBatch(edge_index=[2, 462], y=[10, 1], frac_coords=[50, 3], atom_types=[50], lengths=[10, 3], angles=[10, 3], to_jimages=[462, 3], num_atoms=[10], num_bonds=[10], num_nodes=50, batch=[50], ptr=[11])\n",
      "29 DataBatch(edge_index=[2, 496], y=[10, 1], frac_coords=[47, 3], atom_types=[47], lengths=[10, 3], angles=[10, 3], to_jimages=[496, 3], num_atoms=[10], num_bonds=[10], num_nodes=47, batch=[47], ptr=[11])\n",
      "30 DataBatch(edge_index=[2, 352], y=[10, 1], frac_coords=[39, 3], atom_types=[39], lengths=[10, 3], angles=[10, 3], to_jimages=[352, 3], num_atoms=[10], num_bonds=[10], num_nodes=39, batch=[39], ptr=[11])\n",
      "31 DataBatch(edge_index=[2, 454], y=[10, 1], frac_coords=[45, 3], atom_types=[45], lengths=[10, 3], angles=[10, 3], to_jimages=[454, 3], num_atoms=[10], num_bonds=[10], num_nodes=45, batch=[45], ptr=[11])\n",
      "32 DataBatch(edge_index=[2, 452], y=[10, 1], frac_coords=[45, 3], atom_types=[45], lengths=[10, 3], angles=[10, 3], to_jimages=[452, 3], num_atoms=[10], num_bonds=[10], num_nodes=45, batch=[45], ptr=[11])\n",
      "33 DataBatch(edge_index=[2, 456], y=[10, 1], frac_coords=[43, 3], atom_types=[43], lengths=[10, 3], angles=[10, 3], to_jimages=[456, 3], num_atoms=[10], num_bonds=[10], num_nodes=43, batch=[43], ptr=[11])\n",
      "34 DataBatch(edge_index=[2, 502], y=[10, 1], frac_coords=[46, 3], atom_types=[46], lengths=[10, 3], angles=[10, 3], to_jimages=[502, 3], num_atoms=[10], num_bonds=[10], num_nodes=46, batch=[46], ptr=[11])\n",
      "35 DataBatch(edge_index=[2, 478], y=[10, 1], frac_coords=[46, 3], atom_types=[46], lengths=[10, 3], angles=[10, 3], to_jimages=[478, 3], num_atoms=[10], num_bonds=[10], num_nodes=46, batch=[46], ptr=[11])\n",
      "36 DataBatch(edge_index=[2, 440], y=[10, 1], frac_coords=[44, 3], atom_types=[44], lengths=[10, 3], angles=[10, 3], to_jimages=[440, 3], num_atoms=[10], num_bonds=[10], num_nodes=44, batch=[44], ptr=[11])\n",
      "37 DataBatch(edge_index=[2, 358], y=[10, 1], frac_coords=[40, 3], atom_types=[40], lengths=[10, 3], angles=[10, 3], to_jimages=[358, 3], num_atoms=[10], num_bonds=[10], num_nodes=40, batch=[40], ptr=[11])\n",
      "38 DataBatch(edge_index=[2, 562], y=[10, 1], frac_coords=[55, 3], atom_types=[55], lengths=[10, 3], angles=[10, 3], to_jimages=[562, 3], num_atoms=[10], num_bonds=[10], num_nodes=55, batch=[55], ptr=[11])\n",
      "39 DataBatch(edge_index=[2, 462], y=[10, 1], frac_coords=[51, 3], atom_types=[51], lengths=[10, 3], angles=[10, 3], to_jimages=[462, 3], num_atoms=[10], num_bonds=[10], num_nodes=51, batch=[51], ptr=[11])\n",
      "40 DataBatch(edge_index=[2, 494], y=[10, 1], frac_coords=[51, 3], atom_types=[51], lengths=[10, 3], angles=[10, 3], to_jimages=[494, 3], num_atoms=[10], num_bonds=[10], num_nodes=51, batch=[51], ptr=[11])\n",
      "41 DataBatch(edge_index=[2, 488], y=[10, 1], frac_coords=[49, 3], atom_types=[49], lengths=[10, 3], angles=[10, 3], to_jimages=[488, 3], num_atoms=[10], num_bonds=[10], num_nodes=49, batch=[49], ptr=[11])\n",
      "42 DataBatch(edge_index=[2, 560], y=[10, 1], frac_coords=[49, 3], atom_types=[49], lengths=[10, 3], angles=[10, 3], to_jimages=[560, 3], num_atoms=[10], num_bonds=[10], num_nodes=49, batch=[49], ptr=[11])\n",
      "43 DataBatch(edge_index=[2, 406], y=[10, 1], frac_coords=[43, 3], atom_types=[43], lengths=[10, 3], angles=[10, 3], to_jimages=[406, 3], num_atoms=[10], num_bonds=[10], num_nodes=43, batch=[43], ptr=[11])\n",
      "44 DataBatch(edge_index=[2, 538], y=[10, 1], frac_coords=[52, 3], atom_types=[52], lengths=[10, 3], angles=[10, 3], to_jimages=[538, 3], num_atoms=[10], num_bonds=[10], num_nodes=52, batch=[52], ptr=[11])\n",
      "45 DataBatch(edge_index=[2, 412], y=[10, 1], frac_coords=[50, 3], atom_types=[50], lengths=[10, 3], angles=[10, 3], to_jimages=[412, 3], num_atoms=[10], num_bonds=[10], num_nodes=50, batch=[50], ptr=[11])\n",
      "46 DataBatch(edge_index=[2, 480], y=[10, 1], frac_coords=[48, 3], atom_types=[48], lengths=[10, 3], angles=[10, 3], to_jimages=[480, 3], num_atoms=[10], num_bonds=[10], num_nodes=48, batch=[48], ptr=[11])\n",
      "47 DataBatch(edge_index=[2, 420], y=[10, 1], frac_coords=[45, 3], atom_types=[45], lengths=[10, 3], angles=[10, 3], to_jimages=[420, 3], num_atoms=[10], num_bonds=[10], num_nodes=45, batch=[45], ptr=[11])\n",
      "48 DataBatch(edge_index=[2, 484], y=[10, 1], frac_coords=[45, 3], atom_types=[45], lengths=[10, 3], angles=[10, 3], to_jimages=[484, 3], num_atoms=[10], num_bonds=[10], num_nodes=45, batch=[45], ptr=[11])\n",
      "49 DataBatch(edge_index=[2, 436], y=[10, 1], frac_coords=[47, 3], atom_types=[47], lengths=[10, 3], angles=[10, 3], to_jimages=[436, 3], num_atoms=[10], num_bonds=[10], num_nodes=47, batch=[47], ptr=[11])\n",
      "50 DataBatch(edge_index=[2, 450], y=[10, 1], frac_coords=[51, 3], atom_types=[51], lengths=[10, 3], angles=[10, 3], to_jimages=[450, 3], num_atoms=[10], num_bonds=[10], num_nodes=51, batch=[51], ptr=[11])\n",
      "51 DataBatch(edge_index=[2, 368], y=[10, 1], frac_coords=[37, 3], atom_types=[37], lengths=[10, 3], angles=[10, 3], to_jimages=[368, 3], num_atoms=[10], num_bonds=[10], num_nodes=37, batch=[37], ptr=[11])\n",
      "52 DataBatch(edge_index=[2, 388], y=[10, 1], frac_coords=[38, 3], atom_types=[38], lengths=[10, 3], angles=[10, 3], to_jimages=[388, 3], num_atoms=[10], num_bonds=[10], num_nodes=38, batch=[38], ptr=[11])\n",
      "53 DataBatch(edge_index=[2, 488], y=[10, 1], frac_coords=[46, 3], atom_types=[46], lengths=[10, 3], angles=[10, 3], to_jimages=[488, 3], num_atoms=[10], num_bonds=[10], num_nodes=46, batch=[46], ptr=[11])\n",
      "54 DataBatch(edge_index=[2, 554], y=[10, 1], frac_coords=[65, 3], atom_types=[65], lengths=[10, 3], angles=[10, 3], to_jimages=[554, 3], num_atoms=[10], num_bonds=[10], num_nodes=65, batch=[65], ptr=[11])\n",
      "55 DataBatch(edge_index=[2, 422], y=[10, 1], frac_coords=[41, 3], atom_types=[41], lengths=[10, 3], angles=[10, 3], to_jimages=[422, 3], num_atoms=[10], num_bonds=[10], num_nodes=41, batch=[41], ptr=[11])\n",
      "56 DataBatch(edge_index=[2, 440], y=[10, 1], frac_coords=[43, 3], atom_types=[43], lengths=[10, 3], angles=[10, 3], to_jimages=[440, 3], num_atoms=[10], num_bonds=[10], num_nodes=43, batch=[43], ptr=[11])\n",
      "57 DataBatch(edge_index=[2, 432], y=[10, 1], frac_coords=[43, 3], atom_types=[43], lengths=[10, 3], angles=[10, 3], to_jimages=[432, 3], num_atoms=[10], num_bonds=[10], num_nodes=43, batch=[43], ptr=[11])\n",
      "58 DataBatch(edge_index=[2, 390], y=[10, 1], frac_coords=[40, 3], atom_types=[40], lengths=[10, 3], angles=[10, 3], to_jimages=[390, 3], num_atoms=[10], num_bonds=[10], num_nodes=40, batch=[40], ptr=[11])\n",
      "59 DataBatch(edge_index=[2, 418], y=[10, 1], frac_coords=[40, 3], atom_types=[40], lengths=[10, 3], angles=[10, 3], to_jimages=[418, 3], num_atoms=[10], num_bonds=[10], num_nodes=40, batch=[40], ptr=[11])\n",
      "60 DataBatch(edge_index=[2, 396], y=[10, 1], frac_coords=[42, 3], atom_types=[42], lengths=[10, 3], angles=[10, 3], to_jimages=[396, 3], num_atoms=[10], num_bonds=[10], num_nodes=42, batch=[42], ptr=[11])\n",
      "61 DataBatch(edge_index=[2, 444], y=[10, 1], frac_coords=[46, 3], atom_types=[46], lengths=[10, 3], angles=[10, 3], to_jimages=[444, 3], num_atoms=[10], num_bonds=[10], num_nodes=46, batch=[46], ptr=[11])\n",
      "62 DataBatch(edge_index=[2, 422], y=[10, 1], frac_coords=[43, 3], atom_types=[43], lengths=[10, 3], angles=[10, 3], to_jimages=[422, 3], num_atoms=[10], num_bonds=[10], num_nodes=43, batch=[43], ptr=[11])\n",
      "63 DataBatch(edge_index=[2, 502], y=[10, 1], frac_coords=[50, 3], atom_types=[50], lengths=[10, 3], angles=[10, 3], to_jimages=[502, 3], num_atoms=[10], num_bonds=[10], num_nodes=50, batch=[50], ptr=[11])\n",
      "64 DataBatch(edge_index=[2, 48], y=[1, 1], frac_coords=[4, 3], atom_types=[4], lengths=[1, 3], angles=[1, 3], to_jimages=[48, 3], num_atoms=[1], num_bonds=[1], num_nodes=4, batch=[4], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(dataloader):\n",
    "    print(i,d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so dataloader has the data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffcsp.pl_modules.diffusion_w_type import CSPDiffusion, SinusoidalTimeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffcsp.common.utils import log_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffcsp.pl_modules.cspnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffcsp.pl_modules.cspnet import CSPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspdecoder = CSPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function hydra._internal.instantiate._instantiate2.instantiate(config: Any, *args: Any, **kwargs: Any) -> Any>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.utils.instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "import hydra\n",
    "import omegaconf\n",
    "import pytorch_lightning as pl\n",
    "from torch_scatter import scatter\n",
    "from torch_scatter.composite import scatter_softmax\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from diffcsp.common.utils import PROJECT_ROOT\n",
    "from diffcsp.common.data_utils import (\n",
    "    EPSILON, cart_to_frac_coords, mard, lengths_angles_to_volume, lattice_params_to_matrix_torch,\n",
    "    frac_to_cart_coords, min_distance_sqr_pbc)\n",
    "\n",
    "from diffcsp.pl_modules.diff_utils import d_log_p_wrapped_normal\n",
    "\n",
    "MAX_ATOMIC_NUM=100\n",
    "\n",
    "\n",
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        # populate self.hparams with args and kwargs automagically!\n",
    "        self.save_hyperparameters()\n",
    "        if hasattr(self.hparams, \"model\"):\n",
    "            self._hparams = self.hparams.model\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = hydra.utils.instantiate(\n",
    "            self.hparams.optim.optimizer, params=self.parameters(), _convert_=\"partial\"\n",
    "        )\n",
    "        if not self.hparams.optim.use_lr_scheduler:\n",
    "            return [opt]\n",
    "        scheduler = hydra.utils.instantiate(\n",
    "            self.hparams.optim.lr_scheduler, optimizer=opt\n",
    "        )\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "\n",
    "### Model definition\n",
    "\n",
    "class SinusoidalTimeEmbeddings(nn.Module):\n",
    "    \"\"\" Attention is all you need. \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class CSPDiffusion(BaseModule):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.decoder = hydra.utils.instantiate(cspdecoder, latent_dim = self.hparams.latent_dim + self.hparams.time_dim, pred_type = True, smooth = True)\n",
    "        self.beta_scheduler = hydra.utils.instantiate(self.hparams.beta_scheduler)\n",
    "        self.sigma_scheduler = hydra.utils.instantiate(self.hparams.sigma_scheduler)\n",
    "        self.time_dim = self.hparams.time_dim\n",
    "        self.time_embedding = SinusoidalTimeEmbeddings(self.time_dim)\n",
    "        self.keep_lattice = self.hparams.cost_lattice < 1e-5\n",
    "        self.keep_coords = self.hparams.cost_coord < 1e-5\n",
    "        self.p_uncond = self.hparams.p_uncond\n",
    "        self.guide_w = self.hparams.guide_w\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        batch_size = batch.num_graphs\n",
    "        times = self.beta_scheduler.uniform_sample_t(batch_size, self.device)\n",
    "        time_emb = self.time_embedding(times)\n",
    "\n",
    "        alphas_cumprod = self.beta_scheduler.alphas_cumprod[times]\n",
    "        beta = self.beta_scheduler.betas[times]\n",
    "\n",
    "        c0 = torch.sqrt(alphas_cumprod)\n",
    "        c1 = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "        sigmas = self.sigma_scheduler.sigmas[times]\n",
    "        sigmas_norm = self.sigma_scheduler.sigmas_norm[times]\n",
    "\n",
    "        lattices = lattice_params_to_matrix_torch(batch.lengths, batch.angles)\n",
    "        frac_coords = batch.frac_coords\n",
    "\n",
    "        rand_l, rand_x = torch.randn_like(lattices), torch.randn_like(frac_coords)\n",
    "\n",
    "        input_lattice = c0[:, None, None] * lattices + c1[:, None, None] * rand_l\n",
    "        sigmas_per_atom = sigmas.repeat_interleave(batch.num_atoms)[:, None]\n",
    "        sigmas_norm_per_atom = sigmas_norm.repeat_interleave(batch.num_atoms)[:, None]\n",
    "        input_frac_coords = (frac_coords + sigmas_per_atom * rand_x) % 1.\n",
    "\n",
    "        gt_atom_types_onehot = F.one_hot(batch.atom_types - 1, num_classes=MAX_ATOMIC_NUM).float()\n",
    "\n",
    "        rand_t = torch.randn_like(gt_atom_types_onehot)\n",
    "\n",
    "        atom_type_probs = (c0.repeat_interleave(batch.num_atoms)[:, None] * gt_atom_types_onehot + c1.repeat_interleave(batch.num_atoms)[:, None] * rand_t)\n",
    "\n",
    "        if self.keep_coords:\n",
    "            input_frac_coords = frac_coords\n",
    "\n",
    "        if self.keep_lattice:\n",
    "            input_lattice = lattices\n",
    "\n",
    "\n",
    "        # Need to apply property here, but before need to bernoulli sample\n",
    "        property_indicator = torch.bernoulli(torch.ones(batch_size)*(1.-self.p_uncond))\n",
    "        property_indicator = property_indicator.to(self.device)\n",
    "        property_train = torch.squeeze(batch.y)\n",
    "\n",
    "        pred_l, pred_x, pred_t = self.decoder(time_emb, atom_type_probs, input_frac_coords, input_lattice, batch.num_atoms, batch.batch, property_train, property_indicator)\n",
    "\n",
    "        tar_x = d_log_p_wrapped_normal(sigmas_per_atom * rand_x, sigmas_per_atom) / torch.sqrt(sigmas_norm_per_atom)\n",
    "\n",
    "        loss_lattice = F.mse_loss(pred_l, rand_l)\n",
    "        loss_coord = F.mse_loss(pred_x, tar_x)\n",
    "        loss_type = F.mse_loss(pred_t, rand_t)\n",
    "\n",
    "\n",
    "        loss = (\n",
    "            self.hparams.cost_lattice * loss_lattice +\n",
    "            self.hparams.cost_coord * loss_coord + \n",
    "            self.hparams.cost_type * loss_type)\n",
    "\n",
    "        return {\n",
    "            'loss' : loss,\n",
    "            'loss_lattice' : loss_lattice,\n",
    "            'loss_coord' : loss_coord,\n",
    "            'loss_type' : loss_type\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch, band_gap, diff_ratio = 1.0, step_lr = 1e-5):\n",
    "\n",
    "\n",
    "        batch_size = batch.num_graphs\n",
    "\n",
    "        l_T, x_T = torch.randn([batch_size, 3, 3]).to(self.device), torch.rand([batch.num_nodes, 3]).to(self.device)\n",
    "\n",
    "        t_T = torch.randn([batch.num_nodes, MAX_ATOMIC_NUM]).to(self.device)\n",
    "\n",
    "\n",
    "        if self.keep_coords:\n",
    "            x_T = batch.frac_coords\n",
    "\n",
    "        if self.keep_lattice:\n",
    "            l_T = lattice_params_to_matrix_torch(batch.lengths, batch.angles)\n",
    "        \n",
    "\n",
    "        traj = {self.beta_scheduler.timesteps : {\n",
    "            'num_atoms' : batch.num_atoms,\n",
    "            'atom_types' : t_T,\n",
    "            'frac_coords' : x_T % 1.,\n",
    "            'lattices' : l_T\n",
    "        }}\n",
    "\n",
    "        for t in tqdm(range(self.beta_scheduler.timesteps, 0, -1)):\n",
    "\n",
    "            times = torch.full((batch_size, ), t, device = self.device)\n",
    "\n",
    "            time_emb = self.time_embedding(times)\n",
    "            \n",
    "            alphas = self.beta_scheduler.alphas[t]\n",
    "            alphas_cumprod = self.beta_scheduler.alphas_cumprod[t]\n",
    "\n",
    "            sigmas = self.beta_scheduler.sigmas[t]\n",
    "            sigma_x = self.sigma_scheduler.sigmas[t]\n",
    "            sigma_norm = self.sigma_scheduler.sigmas_norm[t]\n",
    "\n",
    "            c0 = 1.0 / torch.sqrt(alphas)\n",
    "            c1 = (1 - alphas) / torch.sqrt(1 - alphas_cumprod)\n",
    "\n",
    "            x_t = traj[t]['frac_coords']\n",
    "            l_t = traj[t]['lattices']\n",
    "            t_t = traj[t]['atom_types']\n",
    "\n",
    "            if self.keep_coords:\n",
    "                x_t = x_T\n",
    "\n",
    "            if self.keep_lattice:\n",
    "                l_t = l_T\n",
    "\n",
    "            # Corrector\n",
    "\n",
    "            rand_l = torch.randn_like(l_T) if t > 1 else torch.zeros_like(l_T)\n",
    "            rand_t = torch.randn_like(t_T) if t > 1 else torch.zeros_like(t_T)\n",
    "            rand_x = torch.randn_like(x_T) if t > 1 else torch.zeros_like(x_T)\n",
    "\n",
    "            step_size = step_lr * (sigma_x / self.sigma_scheduler.sigma_begin) ** 2\n",
    "            std_x = torch.sqrt(2 * step_size)\n",
    "            # with context\n",
    "            pred_l1, pred_x1, pred_t1 = self.decoder(time_emb, t_t, x_t, l_t, batch.num_atoms, batch.batch, band_gap, torch.ones(batch_size).to(self.device))\n",
    "            pred_x1 = pred_x1 * torch.sqrt(sigma_norm)\n",
    "            # without context\n",
    "            pred_l2, pred_x2, pred_t2 = self.decoder(time_emb, t_t, x_t, l_t, batch.num_atoms, batch.batch, band_gap, torch.zeros(batch_size).to(self.device))\n",
    "            pred_x2 = pred_x2 * torch.sqrt(sigma_norm)\n",
    "            \n",
    "            # weighted score\n",
    "            pred_x = (1+self.guide_w)*pred_x1 - self.guide_w*pred_x2\n",
    "            x_t_minus_05 = x_t - step_size * pred_x + std_x * rand_x if not self.keep_coords else x_t\n",
    "\n",
    "            l_t_minus_05 = l_t\n",
    "\n",
    "            t_t_minus_05 = t_t\n",
    "\n",
    "\n",
    "            # Predictor\n",
    "\n",
    "            rand_l = torch.randn_like(l_T) if t > 1 else torch.zeros_like(l_T)\n",
    "            rand_t = torch.randn_like(t_T) if t > 1 else torch.zeros_like(t_T)\n",
    "            rand_x = torch.randn_like(x_T) if t > 1 else torch.zeros_like(x_T)\n",
    "\n",
    "            adjacent_sigma_x = self.sigma_scheduler.sigmas[t-1] \n",
    "            step_size = (sigma_x ** 2 - adjacent_sigma_x ** 2)\n",
    "            std_x = torch.sqrt((adjacent_sigma_x ** 2 * (sigma_x ** 2 - adjacent_sigma_x ** 2)) / (sigma_x ** 2))   \n",
    "\n",
    "            # with context\n",
    "            pred_l1, pred_x1, pred_t1 = self.decoder(time_emb, t_t, x_t, l_t, batch.num_atoms, batch.batch, band_gap, torch.ones(batch_size).to(self.device))\n",
    "            pred_x1 = pred_x1 * torch.sqrt(sigma_norm)\n",
    "            # without context\n",
    "            pred_l2, pred_x2, pred_t2 = self.decoder(time_emb, t_t, x_t, l_t, batch.num_atoms, batch.batch, band_gap, torch.zeros(batch_size).to(self.device))\n",
    "            pred_x2 = pred_x2 * torch.sqrt(sigma_norm)\n",
    "\n",
    "            ## weighted score\n",
    "            pred_x = (1+self.guide_w)*pred_x1 - self.guide_w*pred_x2\n",
    "            pred_l = (1+self.guide_w)*pred_l1 - self.guide_w*pred_l2\n",
    "            pred_t = (1+self.guide_w)*pred_t1 - self.guide_w*pred_t2\n",
    "\n",
    "            x_t_minus_1 = x_t_minus_05 - step_size * pred_x + std_x * rand_x if not self.keep_coords else x_t\n",
    "\n",
    "            l_t_minus_1 = c0 * (l_t_minus_05 - c1 * pred_l) + sigmas * rand_l if not self.keep_lattice else l_t\n",
    "\n",
    "            t_t_minus_1 = c0 * (t_t_minus_05 - c1 * pred_t) + sigmas * rand_t\n",
    "\n",
    "            traj[t - 1] = {\n",
    "                'num_atoms' : batch.num_atoms,\n",
    "                'atom_types' : t_t_minus_1,\n",
    "                'frac_coords' : x_t_minus_1 % 1.,\n",
    "                'lattices' : l_t_minus_1              \n",
    "            }\n",
    "\n",
    "        traj_stack = {\n",
    "            'num_atoms' : batch.num_atoms,\n",
    "            'atom_types' : torch.stack([traj[i]['atom_types'] for i in range(self.beta_scheduler.timesteps, -1, -1)]).argmax(dim=-1) + 1,\n",
    "            'all_frac_coords' : torch.stack([traj[i]['frac_coords'] for i in range(self.beta_scheduler.timesteps, -1, -1)]),\n",
    "            'all_lattices' : torch.stack([traj[i]['lattices'] for i in range(self.beta_scheduler.timesteps, -1, -1)])\n",
    "        }\n",
    "\n",
    "        return traj[0], traj_stack\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n",
    "\n",
    "        output_dict = self(batch)\n",
    "\n",
    "        loss_lattice = output_dict['loss_lattice']\n",
    "        loss_coord = output_dict['loss_coord']\n",
    "        loss_type = output_dict['loss_type']\n",
    "        loss = output_dict['loss']\n",
    "\n",
    "\n",
    "        self.log_dict(\n",
    "            {'train_loss': loss,\n",
    "            'lattice_loss': loss_lattice,\n",
    "            'coord_loss': loss_coord,\n",
    "            'type_loss': loss_type},\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        if loss.isnan():\n",
    "            return None\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n",
    "\n",
    "        output_dict = self(batch)\n",
    "\n",
    "        log_dict, loss = self.compute_stats(output_dict, prefix='val')\n",
    "\n",
    "        self.log_dict(\n",
    "            log_dict,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n",
    "\n",
    "        output_dict = self(batch)\n",
    "\n",
    "        log_dict, loss = self.compute_stats(output_dict, prefix='test')\n",
    "\n",
    "        self.log_dict(\n",
    "            log_dict,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def compute_stats(self, output_dict, prefix):\n",
    "\n",
    "        loss_lattice = output_dict['loss_lattice']\n",
    "        loss_coord = output_dict['loss_coord']\n",
    "        loss_type = output_dict['loss_type']\n",
    "        loss = output_dict['loss']\n",
    "\n",
    "        log_dict = {\n",
    "            f'{prefix}_loss': loss,\n",
    "            f'{prefix}_lattice_loss': loss_lattice,\n",
    "            f'{prefix}_coord_loss': loss_coord,\n",
    "            f'{prefix}_type_loss': loss_type,\n",
    "        }\n",
    "\n",
    "        return log_dict, loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        # populate self.hparams with args and kwargs automagically!\n",
    "        self.save_hyperparameters()\n",
    "        if hasattr(self.hparams, \"model\"):\n",
    "            self._hparams = self.hparams.model\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = hydra.utils.instantiate(\n",
    "            self.hparams.optim.optimizer, params=self.parameters(), _convert_=\"partial\"\n",
    "        )\n",
    "        if not self.hparams.optim.use_lr_scheduler:\n",
    "            return [opt]\n",
    "        scheduler = hydra.utils.instantiate(\n",
    "            self.hparams.optim.lr_scheduler, optimizer=opt\n",
    "        )\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = BaseModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YourBetaScheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Define the necessary hyperparameters manually\u001b[39;00m\n\u001b[1;32m      2\u001b[0m hparams \u001b[39m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlatent_dim\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m128\u001b[39m,  \u001b[39m# Example value\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtime_dim\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m64\u001b[39m,  \u001b[39m# Example value\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcost_lattice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-6\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcost_coord\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-6\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcost_type\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-6\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mp_uncond\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.1\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mguide_w\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.5\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Mocked classes or replace them with actual instantiated objects\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder\u001b[39m\u001b[39m'\u001b[39m: CSPNet,  \u001b[39m# Adjust as needed\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbeta_scheduler\u001b[39m\u001b[39m'\u001b[39m: YourBetaScheduler(),  \u001b[39m# Replace with your actual scheduler\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msigma_scheduler\u001b[39m\u001b[39m'\u001b[39m: YourSigmaScheduler(),  \u001b[39m# Replace with your actual scheduler\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39m'\u001b[39m\u001b[39moptim\u001b[39m\u001b[39m'\u001b[39m: {\n\u001b[1;32m     15\u001b[0m         \u001b[39m'\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam,  \u001b[39m# Use actual optimizer class\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         \u001b[39m'\u001b[39m\u001b[39muse_lr_scheduler\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mFalse\u001b[39;00m,  \u001b[39m# Example: No learning rate scheduler\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlr_scheduler\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m  \u001b[39m# Optional if `use_lr_scheduler` is False\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     }\n\u001b[1;32m     19\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YourBetaScheduler' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the necessary hyperparameters manually\n",
    "hparams = {\n",
    "    'latent_dim': 128,  # Example value\n",
    "    'time_dim': 64,  # Example value\n",
    "    'cost_lattice': 1e-6,\n",
    "    'cost_coord': 1e-6,\n",
    "    'cost_type': 1e-6,\n",
    "    'p_uncond': 0.1,\n",
    "    'guide_w': 0.5,\n",
    "    # Mocked classes or replace them with actual instantiated objects\n",
    "    'decoder': CSPNet,  # Adjust as needed\n",
    "    'beta_scheduler': YourBetaScheduler(),  # Replace with your actual scheduler\n",
    "    'sigma_scheduler': YourSigmaScheduler(),  # Replace with your actual scheduler\n",
    "    'optim': {\n",
    "        'optimizer': torch.optim.Adam,  # Use actual optimizer class\n",
    "        'use_lr_scheduler': False,  # Example: No learning rate scheduler\n",
    "        'lr_scheduler': None  # Optional if `use_lr_scheduler` is False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AttributeDict' object has no attribute 'latent_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/fermat/lib/python3.12/site-packages/lightning_fabric/utilities/data.py:487\u001b[0m, in \u001b[0;36mAttributeDict.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[1;32m    488\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'latent_dim'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cspdif \u001b[39m=\u001b[39m CSPDiffusion()\n",
      "Cell \u001b[0;32mIn[50], line 73\u001b[0m, in \u001b[0;36mCSPDiffusion.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m hydra\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39minstantiate(cspdecoder, latent_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhparams\u001b[39m.\u001b[39;49mlatent_dim \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mtime_dim, pred_type \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, smooth \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_scheduler \u001b[39m=\u001b[39m hydra\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39minstantiate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mbeta_scheduler)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma_scheduler \u001b[39m=\u001b[39m hydra\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39minstantiate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39msigma_scheduler)\n",
      "File \u001b[0;32m~/miniconda3/envs/fermat/lib/python3.12/site-packages/lightning_fabric/utilities/data.py:489\u001b[0m, in \u001b[0;36mAttributeDict.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[1;32m    488\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AttributeDict' object has no attribute 'latent_dim'"
     ]
    }
   ],
   "source": [
    "cspdif = CSPDiffusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cfg: DictConfig) -> None:\n",
    "\n",
    "    if cfg.train.deterministic:\n",
    "        seed_everything(cfg.train.random_seed)\n",
    "\n",
    "    if cfg.train.pl_trainer.fast_dev_run:\n",
    "        hydra.utils.log.info(\n",
    "            f\"Debug mode <{cfg.train.pl_trainer.fast_dev_run=}>. \"\n",
    "            f\"Forcing debugger friendly configuration!\"\n",
    "        )\n",
    "        # Debuggers don't like GPUs nor multiprocessing\n",
    "        cfg.train.pl_trainer.gpus = 0\n",
    "        cfg.data.datamodule.num_workers.train = 0\n",
    "        cfg.data.datamodule.num_workers.val = 0\n",
    "        cfg.data.datamodule.num_workers.test = 0\n",
    "\n",
    "        # Switch wandb mode to offline to prevent online logging\n",
    "        cfg.logging.wandb.mode = \"offline\"\n",
    "    print('PAST 1')\n",
    "    # Hydra run directory\n",
    "    hydra_dir = Path(HydraConfig.get().run.dir)\n",
    "\n",
    "    # Instantiate datamodule\n",
    "    hydra.utils.log.info(f\"Instantiating <{cfg.data.datamodule._target_}>\")\n",
    "    datamodule: pl.LightningDataModule = hydra.utils.instantiate(\n",
    "        cfg.data.datamodule, _recursive_=False\n",
    "    )\n",
    "    print('PAST 2')\n",
    "    # Instantiate model\n",
    "    hydra.utils.log.info(f\"Instantiating <{cfg.model._target_}>\")\n",
    "    model: pl.LightningModule = hydra.utils.instantiate(\n",
    "        cfg.model,\n",
    "        optim=cfg.optim,\n",
    "        data=cfg.data,\n",
    "        logging=cfg.logging,\n",
    "        _recursive_=False,\n",
    "    )\n",
    "\n",
    "    # Pass scaler from datamodule to model\n",
    "    hydra.utils.log.info(f\"Passing scaler from datamodule to model <{datamodule.scaler}>\")\n",
    "    if datamodule.scaler is not None:\n",
    "        model.lattice_scaler = datamodule.lattice_scaler.copy()\n",
    "        model.scaler = datamodule.scaler.copy()\n",
    "    torch.save(datamodule.lattice_scaler, hydra_dir / 'lattice_scaler.pt')\n",
    "    torch.save(datamodule.scaler, hydra_dir / 'prop_scaler.pt')\n",
    "    # Instantiate the callbacks\n",
    "    callbacks: List[Callback] = build_callbacks(cfg=cfg)\n",
    "    print('PAST 3')\n",
    "    # Logger instantiation/configuration\n",
    "    wandb_logger = None\n",
    "    if \"wandb\" in cfg.logging:\n",
    "        hydra.utils.log.info(\"Instantiating <WandbLogger>\")\n",
    "        wandb_config = cfg.logging.wandb\n",
    "        wandb_logger = WandbLogger(\n",
    "            **wandb_config,\n",
    "            settings=wandb.Settings(start_method=\"fork\"),\n",
    "            tags=cfg.core.tags,\n",
    "        )\n",
    "        hydra.utils.log.info(\"W&B is now watching <{cfg.logging.wandb_watch.log}>!\")\n",
    "        wandb_logger.watch(\n",
    "            model,\n",
    "            log=cfg.logging.wandb_watch.log,\n",
    "            log_freq=cfg.logging.wandb_watch.log_freq,\n",
    "        )\n",
    "\n",
    "    # Store the YaML config separately into the wandb dir\n",
    "    yaml_conf: str = OmegaConf.to_yaml(cfg=cfg)\n",
    "    (hydra_dir / \"hparams.yaml\").write_text(yaml_conf)\n",
    "\n",
    "    # Load checkpoint (if exist)\n",
    "    ckpts = list(hydra_dir.glob('*.ckpt'))\n",
    "    if len(ckpts) > 0:\n",
    "        ckpt_epochs = np.array([int(ckpt.parts[-1].split('-')[0].split('=')[1]) for ckpt in ckpts])\n",
    "        ckpt = str(ckpts[ckpt_epochs.argsort()[-1]])\n",
    "        hydra.utils.log.info(f\"found checkpoint: {ckpt}\")\n",
    "    else:\n",
    "        ckpt = None\n",
    "          \n",
    "    hydra.utils.log.info(\"Instantiating the Trainer\")\n",
    "    print('PAST 4')\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=hydra_dir,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=callbacks,\n",
    "        deterministic=cfg.train.deterministic,\n",
    "        check_val_every_n_epoch=cfg.logging.val_check_interval,\n",
    "        # progress_bar_refresh_rate=cfg.logging.progress_bar_refresh_rate,\n",
    "        # resume_from_checkpoint=ckpt,\n",
    "        **cfg.train.pl_trainer,\n",
    "    )\n",
    "\n",
    "    log_hyperparameters(trainer=trainer, model=model, cfg=cfg)\n",
    "\n",
    "    hydra.utils.log.info(\"Starting training!\")\n",
    "    print('PAST 5')\n",
    "    trainer.fit(model=model, datamodule=datamodule)\n",
    "\n",
    "    hydra.utils.log.info(\"Starting testing!\")\n",
    "    trainer.test(datamodule=datamodule)\n",
    "\n",
    "    # Logger closing to release resources/avoid multi-run conflicts\n",
    "    if wandb_logger is not None:\n",
    "        wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/44383126/ipykernel_3793186/170551896.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=str(PROJECT_ROOT / \"conf\"), config_name=\"default\")\n"
     ]
    }
   ],
   "source": [
    "@hydra.main(config_path=str(PROJECT_ROOT / \"conf\"), config_name=\"default\")\n",
    "def main(cfg: omegaconf.DictConfig):\n",
    "    run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fermat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
